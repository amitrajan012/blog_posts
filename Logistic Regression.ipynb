{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.3em;\n",
       "line-height:1.4em;\n",
       "padding-left:1.5em;\n",
       "padding-right:1.5em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.3em;\n",
    "line-height:1.4em;\n",
    "padding-left:1.5em;\n",
    "padding-right:1.5em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Logistic Regression</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a classification setting, <b>logistic regression</b> models the probability of a response $Y$ belonging to a particaular category. A simple linear regression can not be used for classification as the output of a linear regression can have a range that goes from $-\\infty$ to $\\infty$ (we need to find the values in the range [0, 1]). Instead, we can transform the output of linear regression such that the output is confined in the range [0, 1]. For this, a <b>logistic function</b> which is given below can be used.\n",
    "\n",
    "$$p(X) = \\frac{e^{\\beta_0 + \\beta X}}{1 + e^{\\beta_0 + \\beta X}}$$\n",
    "\n",
    "Manipulating the above function and taking the log, we get\n",
    "\n",
    "$$log\\bigg( \\frac{p(X)}{1-p(X)} \\bigg) = \\beta_0 + \\beta X$$\n",
    "\n",
    "The left hand side expression is called as <b>log-odds</b> of <b>logit</b>. For a logistic regression, <b>logit is linear in $X$</b>.\n",
    "\n",
    "A method of <b>maximum-likelihood</b> is used to fit the logistic regression model. In maximum-likelihood, we seek estimates of $\\beta_0$ and $\\beta$ such that the predicted probabilities corresponds as closely as possible to the observed individual probabilities. The <b>likelihood-function</b> is given as:\n",
    "\n",
    "$$L(\\beta_0, \\beta) = \\prod_{i:y_i=1}p(x_i) \\prod_{i:y_i=0}(1 - p(x_i)) = \\prod_{i=1}^{n}p(x_i)^{y_i} (1- p(x_i))^{1 - y_i}$$\n",
    "\n",
    "In maximum-likelihood, our goal is to maximize the likelihood-function. As logarithm is an increasing function, we can transform the likelihood-function by taking logarithm and our objective remains unaltered. Hence, the <b>log-likelihood</b> (after taking the logarithm of the likelihood-function) is given as:\n",
    "\n",
    "$$l(\\beta_0, \\beta) = log(L(\\beta_0, \\beta)) = \\sum_{i=1}^{n} y_i log(p(x_i)) + (1- y_i)(1 - log(p(x_i)))$$\n",
    "\n",
    "On further simplification, we get\n",
    "\n",
    "$$l(\\beta_0, \\beta) = \\sum_{i=1}^{n} log(1 - p(x_i)) + \\sum_{i=1}^{n} y_i log \\frac{p(x_i)}{1 - p(x_i)} = \n",
    "-\\sum_{i=1}^{n} log(1 + e^{\\beta_0 + \\beta x_i}) + \\sum_{i=1}^{n} y_i (\\beta_0 + \\beta x_i)$$\n",
    "\n",
    "To find the maximum likelihood estimates, we need to take the partial derivatives of log-likelihood function with respect to different parameters and set them to 0. Taking the derivative with respect to one component of $\\beta$ (say it as $\\beta_j$), we get\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\beta_j} = -\\sum_{i=1}^{n} \\frac{1}{e^{\\beta_0 + \\beta x_i}} e^{\\beta_0 + \\beta x_i} x_{ij}+ \\sum_{i=1}^{n} y_i x_{ij} = \\sum_{i=1}^{n} (y_i - p(x_i; \\beta_0, \\beta)) x_{ij}$$\n",
    "\n",
    "As exponential function is a <b>transcendental function</b> (as it goes beyond the limit of algebra in that it can not be expressed in terms of a finite sequence of the algebric operations (addition, multiplication,...)), the above equation is a <b>transcendental equation</b>. A transcendental equations do not have <b>closed-form solutions</b> (an expression that can be evaluated in a finite number of mathematical operations). However this can be solved approximately to get a numerical solution.\n",
    "\n",
    "<b>Newton-Raphson</b> method can be used to find an approximate solution for the above mentioned optimization problem. Let us first assume a simpler case of minimizing a function of one scalar variable, denoted as $f(\\beta)$. We have to find the location of the <b>global-minima</b> $\\beta^{*}$. For $\\beta^{*}$ to be a global-minima, the first-derivative at $\\beta^{*}$ should be 0 and the second-derivative should be positive. Doing a <b>Taylor Series</b> expansion near minima, we get\n",
    "\n",
    "$$f(\\beta) \\approx f(\\beta^{*}) + (\\beta - \\beta^{*})f^{'}(\\beta^{*}) + \\frac{(\\beta - \\beta^{*})^2}{2} f^{\"}(\\beta^{*}) + ... = f(\\beta^{*}) + \\frac{(\\beta - \\beta^{*})^2}{2} f^{\"}(\\beta^{*})$$\n",
    "\n",
    "The last expression comes due to the fact that the first-derivative at the global-minima, i.e. $f^{'}(\\beta^{*})$ is 0. Hence, instead of minimizing the original function, we can minimize the <b>quadratic approximation</b> of the original function. \n",
    "\n",
    "In the Newton-Raphson method, we guess an initial point (let it be $\\beta^{0}$). If this point is close to the minima, we can take a second order taylor expansion around $\\beta^{0}$ and it will still be close to the original expression:\n",
    "\n",
    "$$f(\\beta) \\approx f(\\beta^{0}) + (\\beta - \\beta^{0})f^{'}(\\beta^{0}) + \\frac{(\\beta - \\beta^{*0})^2}{2} f^{\"}(\\beta^{0})$$\n",
    "\n",
    "So, instead of minimizing the original expression, we minimize the right hand side, which is the second-order taylor expansion of the original expression around $\\beta^{0}$. Taking the derivative of the right hand side expression and setting it equal to 0, we get\n",
    "\n",
    "$$f^{'}(\\beta^{0}) + (\\beta - \\beta^{0}) f^{\"}(\\beta^{0}) = 0$$\n",
    "\n",
    "This gives us a new guess $\\beta^{1}$ for the global-minima $\\beta^{*}$ as\n",
    "\n",
    "$$\\beta^{1} = \\beta^{0} - \\frac{f^{'}(\\beta^{0})}{f^{\"}(\\beta^{0})}$$\n",
    "\n",
    "After repeating the process till $n$ iterations, we get the $(n+1)^{th}$ guess as:\n",
    "\n",
    "$$\\beta^{n+1} = \\beta^{n} - \\frac{f^{'}(\\beta^{n})}{f^{\"}(\\beta^{n})}$$\n",
    "\n",
    "This gives us the approximation for global-minima."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
